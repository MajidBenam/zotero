{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pprint\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-financing",
   "metadata": {},
   "source": [
    "### Feed Zotero with Google Scholar Data\n",
    "\n",
    "- For now, I am doing this with endnote files (`.enw`) files, but there might be other ways that give us more information, but I don't actually think so.\n",
    "- In zotero Desktop: Go to File -> import and bam, you are done.\n",
    "- Each time you do this, a new collection will be created with the same name as the name if the file.\n",
    "- You can easily drag and drop the collection and put it in any of the folders in the Seshat Databank and all the new items will also be included in the next export.\n",
    "- At this point, I believe it makes more sense to export JSON files from different collections separately. This way, we can already keep some meta data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-listing",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTES\n",
    "\n",
    "##### We need to do things from Python\n",
    "- I should try to avoid doing transactions from PgAdmin, I should be able to read JSON data, manipulate and compare things with each other all in one python file (not `models.py`, in `models.py` we put the first level validations, the next levels can go into another file or maybe also there. I don't know) and add new rows to our database with appropriate tags.\n",
    "    - For example: I might need to check the similarity level between a potential zotero title and a search term and populate that particular column in that particular table in the dtabase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-pharmaceutical",
   "metadata": {},
   "source": [
    "### Extract JSON from Zotero\n",
    "\n",
    "The idea is to have a thorough Zotero collection at hand (in JSON format).\n",
    "\n",
    "We can get the JSON from Zotero at any point in time: \n",
    "- go to Zotero Desktop\n",
    "- right click on the big library and choose `Export Library`\n",
    "- choose `CSL JSON` as the format.\n",
    "\n",
    "The output file will be a pretty normal JSON file, with key-value pairs inside a list (a list of many dics). We believe we don't have to unnecessarily convert JSON to CSV, unless if we want to put data in SSQL and we need CSV, for thatwe can use the old python file: `from_zotero_to_postgres_prep.py`\n",
    "\n",
    "**Note**: We can also do the export on any inner folder and save the results in a separate JSON file. We might need to know the reference we are looking for is found in WHICH folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-dancing",
   "metadata": {},
   "source": [
    "What is important is that there is a smooth flow from Zotero to SSQL and vice versa.\n",
    "\n",
    "What we want to do at this stage, are some simple checks as well as general big chunks check.\n",
    "\n",
    "#### Small checks:\n",
    "- how does the `CSL JSON` file above treats duplicates and unfilled refs?\n",
    "Let's check it now. **Result:** Yes, duplicates are actually counted extra, so we should take care of duplicates before using the databses and its unique references.\n",
    "- Unfiled Items are also included in the JSON file, athough they might be half-filled, but it is good that we have a list of them on the Zotero. Bad thing: the JSON file does not reflect that. That menas you should never think that all the columns are there at our service. We should make sure we have authors or years or ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-newark",
   "metadata": {},
   "source": [
    "# Small Sample Project\n",
    "\n",
    "We wanna make a small toy website with very small data to check these:\n",
    "- Can we do everything in `models.py` files (for example can we import similar functions and do it there? I don't see any reason why not)\n",
    "\n",
    "The data model (to be put in Django) is like this:\n",
    "\n",
    "| Polity (link) | Old_Ref | year | title | percent | GS_num_stars(invisible) | Final_check | RA Pick (zotero/Year_title) | \n",
    "| --- | --- | --- | --- | --- | :---: | --- | --- |\n",
    "| AfDurrn | Intro to C++ | 2001 |into to c | 74% | 2 | Approved | | \n",
    "| AfDurrn | Intro to Java | 2001 | into to J |74% | 10 |   Not sure | | \n",
    "| AfDurrn | Intro to Matlab | 2004 | into to c | 64% | 1 | New Pick | 2001_introoo to c++ | \n",
    "| AfDurrn | Intro to C | 2010 | into to c | 77% | 3 |  Approved | | \n",
    "\n",
    "conditions:\n",
    "- if New_Pick: new_year `and` new_title must be filled, `OR` new_zotero_link must be filled.\n",
    "We should start a brand new project in Django with only one Data model for: `Reference Clean up`:\n",
    "\n",
    "Before we can see anything on the admin page, we should populate the database (based on the data model we define) with the data from our ZOTERO JSON file, either through:\n",
    "- Creating csv files and importing in PgAdmin or ...\n",
    "- Creating things and puting them in sqlite for now. why not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "portable-blanket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5727\n"
     ]
    }
   ],
   "source": [
    "helping_dic = {}\n",
    "\n",
    "# input file (containing ALL zotero data)\n",
    "goodname = \"Seshat_Databank_Refs_2_Dec_2021_3.json\"\n",
    "# the above file can be used by itself, we don't have to convert it to CSV.\n",
    "\n",
    "# we want to save the data in a form that is easier to be used in SSQL\n",
    "with open(goodname, \"r\") as jsonfile:\n",
    "    helping_dic = json.load(jsonfile)\n",
    "    \n",
    "print(len(helping_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-optics",
   "metadata": {},
   "source": [
    "5721 without merged duplocate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-intensity",
   "metadata": {},
   "source": [
    "# Postgres or SQLite: Postgres\n",
    "\n",
    "- I can use the same old code and create csv with tab separated values (read from JSON) and do the job through PgAdmin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
